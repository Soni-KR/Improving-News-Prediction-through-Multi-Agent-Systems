{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IUmGwJawjXY"
   },
   "source": [
    "**1**-INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "76fpCPHb53u2",
    "outputId": "21eba1db-5674-4784-b21e-9f3014806624"
   },
   "outputs": [],
   "source": [
    "!pip install torch torch_geometric pandas networkx scikit-learn --quiet\n",
    "# Required for Groq API\n",
    "!pip install groq\n",
    "\n",
    "# Torch + PyTorch Geometric\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install torch==2.4.0 torch-geometric==2.5.3 torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cpu.html\n",
    "#!pip install  torch-geometric --quiet\n",
    "\n",
    "\n",
    "# Core graph + ML + data tools\n",
    "!pip install networkx pandas numpy scikit-learn\n",
    "\n",
    "# Optional: if you use progress bars\n",
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IZbZRZxwyoo"
   },
   "source": [
    "2-**AGENT1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19K0Kwnn5-2p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "start_row = 0\n",
    "end_row = 2000\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('agent1_history.txt', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Groq client\n",
    "GROQ_API_KEY = \"\"\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Cache configuration\n",
    "CACHE_FILE = \"llm_cache.json\"\n",
    "CACHE_EXPIRY_SECONDS = 7 * 24 * 60 * 60  # 7 days\n",
    "\n",
    "# Normalization\n",
    "DISEASE_NORMALIZATION = {\n",
    "    'corona': 'كورونا',\n",
    "    'Covid-19': 'كورونا',\n",
    "    'coronavirus': 'كورونا',\n",
    "    'فيروس كورونا': 'كورونا'\n",
    "}\n",
    "EXCLUDED_TERMS = {'الصحية', 'santé', 'health', 'pandémie', 'pandemic', 'cas', 'cases', 'vaccine', 'حالات'}\n",
    "\n",
    "def load_cache() -> Dict:\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "                cache = json.load(f)\n",
    "                current_time = time.time()\n",
    "                return {\n",
    "                    k: v for k, v in cache.items()\n",
    "                    if v.get('timestamp', 0) + CACHE_EXPIRY_SECONDS > current_time\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading cache: {e}\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache: Dict):\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving cache: {e}\")\n",
    "\n",
    "def summarize_article(text: str) -> str:\n",
    "    cache = load_cache()\n",
    "    cache_key = f\"summarize_{hash(text)}\"\n",
    "    if cache_key in cache:\n",
    "        logger.info(f\"Using cached summary for text: {text[:50]}...\")\n",
    "        return cache[cache_key]['summary']\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a health news expert. Your task is to extract country–disease or country–vaccine pairs \"\n",
    "        \"specifically mentioned in the headline below.\\n\\n\"\n",
    "\n",
    "        \"Include only:\\n\"\n",
    "        \"- Specific diseases (e.g., كورونا, إنفلونزا, كوليرا), or\\n\"\n",
    "        \"- Recognized vaccine names (e.g., فايزر, موديرنا, أسترازينيكا) paired with a clearly identifiable country (e.g., البرازيل, أمريكا).\\n\\n\"\n",
    "\n",
    "        \"Normalization rules:\\n\"\n",
    "        \"- All forms of corona (corona, Covid-19, فيروس كورونا) should become: كورونا\\n\"\n",
    "        \"- All vaccine names should become: لقاح\\n\\n\"\n",
    "\n",
    "        \"Exclude:\\n\"\n",
    "        \"- Generic terms (e.g., حالات, الوضع, الصحية)\\n\"\n",
    "        \"- Vague or regional terms like: العالم, \\n\"\n",
    "        \"- Any headline without a clear (country, disease/vaccine) pair\\n\\n\"\n",
    "\n",
    "        \"Expected output:\\n\"\n",
    "        \"- A single line with one or more pairs in the format: 'country disease' or 'country لقاح', separated by spaces.\\n\"\n",
    "        \"- If no valid pair exists, return an empty string.\\n\"\n",
    "        \"- Do NOT add any explanation or extra text.\\n\\n\"\n",
    "\n",
    "        f\"Title: {text}\\nResult:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        pairs = result.split()\n",
    "        cleaned_pairs = []\n",
    "        for i in range(0, len(pairs), 2):\n",
    "            if i + 1 < len(pairs):\n",
    "                country, name = pairs[i], pairs[i + 1]\n",
    "                if name in DISEASE_NORMALIZATION:\n",
    "                    name = DISEASE_NORMALIZATION[name]\n",
    "                if name and country and name not in EXCLUDED_TERMS and country not in {'العالم', 'أوروبا', ''}:\n",
    "                    cleaned_pairs.append(f\"{country} {name}\")\n",
    "        cleaned_result = \" \".join(cleaned_pairs)\n",
    "        cache[cache_key] = {\n",
    "            'summary': cleaned_result,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        save_cache(cache)\n",
    "        logger.info(f\"Generated summary for: {text[:50]}... -> {cleaned_result}\")\n",
    "        return cleaned_result\n",
    "    except Exception as e:\n",
    "        logger.error(f\"LLM summarization error for text {text[:50]}... : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def batch_summarize(texts):\n",
    "    return [summarize_article(text) for text in texts]\n",
    "\n",
    "logger.info(\"Loading inputfilegraph.csv\")\n",
    "try:\n",
    "    df = pd.read_csv(\"inputfilegraph.csv\")\n",
    "    logger.info(f\"File has {len(df)} rows. Processing rows {start_row} to {end_row}\")\n",
    "    df = df.iloc[start_row:end_row].copy()\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"File inputfilegraph.csv not found\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading file: {e}\")\n",
    "    raise\n",
    "\n",
    "if 'New_Text' not in df.columns:\n",
    "    logger.error(\"Missing 'New_Text' column in inputfilegraph.csv\")\n",
    "    raise ValueError(\"Missing column 'New_Text'\")\n",
    "\n",
    "start_time = time.time()\n",
    "summary_file = \"summarized_inputfilegraph.csv\"\n",
    "logger.info(\"Running Agent 1 to summarize articles\")\n",
    "\n",
    "if os.path.exists(summary_file):\n",
    "    try:\n",
    "        summaries_df = pd.read_csv(summary_file, encoding='utf-8')\n",
    "        if 'Summary' in summaries_df.columns:\n",
    "            logger.info(f\"Loaded summaries from {summary_file}\")\n",
    "            summaries_df = summaries_df.iloc[start_row:end_row].copy()\n",
    "            df = df.iloc[:len(summaries_df)]\n",
    "            df['Summary'] = summaries_df['Summary']\n",
    "            df['Original'] = summaries_df['Original'] if 'Original' in summaries_df.columns else df['New_Text']\n",
    "        else:\n",
    "            raise Exception(\"Missing 'Summary' column in summary file.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Problem loading summary file: {e}. Regenerating summaries.\")\n",
    "        texts = df['New_Text'].fillna(\"\").astype(str).tolist()\n",
    "        summaries = []\n",
    "        for i in range(0, len(texts), BATCH_SIZE):\n",
    "            summaries.extend(batch_summarize(texts[i:i+BATCH_SIZE]))\n",
    "            time.sleep(1.0)\n",
    "        df['Summary'] = summaries\n",
    "        df['Original'] = df['New_Text']\n",
    "        df[['Original', 'Summary']].to_csv(summary_file, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Summaries saved to {summary_file}\")\n",
    "else:\n",
    "    logger.info(f\"Generating summaries for {len(df)} articles\")\n",
    "    texts = df['New_Text'].fillna(\"\").astype(str).tolist()\n",
    "    summaries = []\n",
    "    for i in range(0, len(texts), BATCH_SIZE):\n",
    "        summaries.extend(batch_summarize(texts[i:i+BATCH_SIZE]))\n",
    "        time.sleep(1.0)\n",
    "    df['Summary'] = summaries\n",
    "    df['Original'] = df['New_Text']\n",
    "    df[['Original', 'Summary']].to_csv(summary_file, index=False, encoding='utf-8')\n",
    "    logger.info(f\"Summaries saved to {summary_file}\")\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "logger.info(f\"Total execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSeLDWT8w9l9"
   },
   "source": [
    "-MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUhGg0KNg4QS",
    "outputId": "afdbee88-84ef-4963-d7a1-cb3b4b9d2aae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_paths = [\n",
    "    \"0_2000.csv\",\n",
    "    \"2000_4000.csv\",\n",
    "    \"4000_6000.csv\",\n",
    "    \"6000_8000.csv\",\n",
    "    \"8000_10000.csv\"\n",
    "]\n",
    "\n",
    "dfs = [pd.read_csv(fp, encoding='utf-8') for fp in file_paths]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: check if row count matches original dataset\n",
    "print(\"Merged rows:\", len(merged_df))\n",
    "\n",
    "merged_df.to_csv(\"merged_summaries.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHV3JcYoxCDJ"
   },
   "source": [
    "3-BERT SCORE (2 PARTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hH78-Kyng5Gu",
    "outputId": "4d330c81-ecff-43e3-b740-545e7cfc1e8a"
   },
   "outputs": [],
   "source": [
    "#code Bertscore(arabic + other)\n",
    "!pip install bert-score\n",
    "import pandas as pd\n",
    "import logging\n",
    "from bert_score import score\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "try:\n",
    "    df = pd.read_csv(\"merged_summaries.csv\")\n",
    "    logger.info(\"Cleaned CSV file loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"The cleaned CSV file was not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error while loading the CSV file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Prepare the texts\n",
    "df['Original'] = df['Original'].fillna(\"\").astype(str)\n",
    "df['Summary'] = df['Summary'].fillna(\"\").astype(str)\n",
    "original_texts = df['Original'].tolist()\n",
    "summaries = df['Summary'].tolist()\n",
    "\n",
    "# Compute BERTScore\n",
    "logger.info(\"Calculating BERTScore...\")\n",
    "P, R, F1 = score(summaries, original_texts, lang=\"ar\", model_type=\"bert-base-multilingual-cased\")\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "# Display the results directly\n",
    "print(\"\\nBERTScore Results:\")\n",
    "print(f\"Average BERTScore F1: {avg_f1:.4f}\")\n",
    "logger.info(\"BERTScore metric successfully displayed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329,
     "referenced_widgets": [
      "21a1c28408704ee588219f0d95f7be6a",
      "83cff5562a004f1299627882eba9ef1d",
      "6c679aff890e483ab167056e3647ede4",
      "1b4d89c4c8d54fcb90d6644f8188e903",
      "5a5b7a5ccd144c6380463e32d7c26dba",
      "12a7636dcb474b0b84782164b94f290a",
      "87699e93322348ee980bb8d7b8b538e8",
      "f100ac783e3d4b6ea5304aa724ac139f",
      "22f769e4ad824d66b4c3ff8c20552ccb",
      "93bfa268659146a1b4eacc623c0886b6",
      "bab7acf7e0be4ea3910e9333496c4845"
     ]
    },
    "id": "46hRJ5nCHTc2",
    "outputId": "328a5ca5-eb27-4e79-c147-78aabdc128ea"
   },
   "outputs": [],
   "source": [
    "#code Bertscore (arabic)\n",
    "import pandas as pd\n",
    "import logging\n",
    "from bert_score import score\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load the cleaned CSV file\n",
    "try:\n",
    "    df = pd.read_csv(\"merged_summaries.csv\")\n",
    "    logger.info(\"Cleaned CSV file loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"The cleaned CSV file was not found.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error while loading the CSV file: {e}\")\n",
    "    raise\n",
    "\n",
    "# Filtrer pour ne garder que les lignes où 'Summary' contient au moins un caractère arabe\n",
    "import re\n",
    "arabic_pattern = re.compile(r'[\\u0600-\\u06FF]')\n",
    "df = df[df['Summary'].apply(lambda x: bool(arabic_pattern.search(str(x))))]\n",
    "logger.info(f\"Nombre de summaries en arabe : {len(df)}\")\n",
    "\n",
    "# Prepare the texts\n",
    "df['Original'] = df['Original'].fillna(\"\").astype(str)\n",
    "df['Summary'] = df['Summary'].fillna(\"\").astype(str)\n",
    "original_texts = df['Original'].tolist()\n",
    "summaries = df['Summary'].tolist()\n",
    "\n",
    "# Compute BERTScore\n",
    "logger.info(\"Calculating BERTScore...\")\n",
    "P, R, F1 = score(summaries, original_texts, lang=\"ar\", model_type=\"bert-base-multilingual-cased\")\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "# Display the results directly\n",
    "print(\"\\nBERTScore Results:\")\n",
    "print(f\"Average BERTScore F1: {avg_f1:.4f}\")\n",
    "logger.info(\"BERTScore metric successfully displayed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MssLq0-xJDt"
   },
   "source": [
    "4-AGENT 1 METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VGc_4UCcwiAP",
    "outputId": "da37a661-836e-4c5a-9481-b087b3ebdbb3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"agent1_ground_truth_template.csv\")  # Replace with your path\n",
    "\n",
    "# Preprocessing: remove anything not Arabic\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    # If not purely Arabic text (letters, spaces only), discard\n",
    "    if not re.fullmatch(r'[\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\s]+', text):\n",
    "        return \"\"\n",
    "    return text.lower()\n",
    "\n",
    "df[\"Clean_Summary\"] = df[\"Summary\"].apply(clean_text)\n",
    "df[\"Clean_Manual\"] = df[\"Manual_Ground_Truth\"].apply(clean_text)\n",
    "\n",
    "# Helper for ROUGE-L\n",
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i] == Y[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
    "    return dp[m][n]\n",
    "\n",
    "# Compute all metrics\n",
    "def compute_all_metrics(summary, reference):\n",
    "    if summary.strip() == \"\" and reference.strip() == \"\":\n",
    "        return {\n",
    "            \"ROUGE-1\": 1.0,\n",
    "            \"ROUGE-L\": 1.0,\n",
    "            \"BLEU\": 1.0,\n",
    "            \"Exact_Match\": 1.0,\n",
    "            \"Jaccard\": 1.0,\n",
    "            \"Token_F1\": 1.0\n",
    "        }\n",
    "\n",
    "    summary_tokens = word_tokenize(summary)\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "\n",
    "    # ROUGE-1\n",
    "    overlap = set(summary_tokens) & set(reference_tokens)\n",
    "    p = len(overlap) / len(summary_tokens) if summary_tokens else 0\n",
    "    r = len(overlap) / len(reference_tokens) if reference_tokens else 0\n",
    "    rouge1_f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    # ROUGE-L\n",
    "    lcs_len = lcs(summary_tokens, reference_tokens)\n",
    "    p_lcs = lcs_len / len(summary_tokens) if summary_tokens else 0\n",
    "    r_lcs = lcs_len / len(reference_tokens) if reference_tokens else 0\n",
    "    rougeL_f1 = 2 * p_lcs * r_lcs / (p_lcs + r_lcs) if p_lcs + r_lcs > 0 else 0\n",
    "\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    bleu = sentence_bleu([reference_tokens], summary_tokens, smoothing_function=smoothie)\n",
    "\n",
    "    # Exact Match\n",
    "    exact = float(summary.strip() == reference.strip())\n",
    "\n",
    "    # Jaccard\n",
    "    union = set(summary_tokens) | set(reference_tokens)\n",
    "    jaccard = len(overlap) / len(union) if union else 0\n",
    "\n",
    "    # Token-level F1\n",
    "    token_f1 = (2 * p * r) / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"ROUGE-1\": rouge1_f1,\n",
    "        \"ROUGE-L\": rougeL_f1,\n",
    "        \"BLEU\": bleu,\n",
    "        \"Exact_Match\": exact,\n",
    "        \"Jaccard\": jaccard,\n",
    "        \"Token_F1\": token_f1\n",
    "    }\n",
    "\n",
    "# Apply metrics\n",
    "metrics = df.apply(lambda row: compute_all_metrics(row[\"Clean_Summary\"], row[\"Clean_Manual\"]), axis=1)\n",
    "metrics_df = pd.DataFrame(metrics.tolist())\n",
    "\n",
    "# Merge and save\n",
    "result_df = pd.concat([df[[\"Original\", \"Summary\", \"Manual_Ground_Truth\"]], metrics_df], axis=1)\n",
    "average_scores = metrics_df.mean()\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv(\"resultll.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "# Print results\n",
    "print(\"\\nAverage Scores:\\n\", average_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im1ZU0AUxNke"
   },
   "source": [
    "5-AGENT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COVexaN1Xphe",
    "outputId": "b17ff9ac-920a-4392-a72d-bc0770ee3b16"
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install -q transformers==4.36.2\n",
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "205c61c2715b451389d9b9783ff07419",
      "753a07fb5af543fda70324b324824931",
      "a119e607aebd47eba23a3edf3d89c191",
      "e2a5573375d84829aecdbcabd4195516",
      "b664827e6a2c478e90df240ef746c477",
      "1952a40d9a174ff9a3fc494002ea9711",
      "c39a48a2dbe0468dba609b7766c50358",
      "177dd6c12ba0440abe44746f059f5b12",
      "16c9c1d87ade4994b11829d10940f285",
      "4020d092f77e443492ef205f1fa60a5a",
      "898ebf82bc3245259039e3c92ad896d2",
      "2756a2c373a14d0383c698445b0468db",
      "7ccf4d3e584b408285a75a710e975976",
      "eaf06add4e39417897600f65ad93c8f3",
      "7dfbd190b7e64b698d8a7bf6818da85a",
      "5482bee9863c47cab2447a5cc19b42d2",
      "6c433f9cad2c402d9c78a61ce1a1e800",
      "415c716569cc4189a9305607c429499c",
      "af4e6f5a7dd04962b8166a61b17e7bf8",
      "d25de8f2c1f646998bfbea0949074cf1",
      "2f1f7d9b60194a65bb8227126d5be401",
      "70e19a7a3ffe492689d664f83529b0ea",
      "4790c66684674b1f92a1323d0bd94d78",
      "1c06694f24104da1ba6015fff99d8a8b",
      "c42959befb2d4b75be2cf3a1012f3e67",
      "f44fc2304db04b159500adaaa74cae95",
      "be8638a79d5049f6b923681ad94c8ed9",
      "438c8cf9abee4bd398a4af3012240fd7",
      "7f3c5091e54841d89bc616ff5fd293ce",
      "e65e884698ad4079bc8ac6694c0d498d",
      "2fbc0391cd9b487eb115522337f1915a",
      "2396e5558d1641b79c5a1bd613d1c9ca",
      "34e05e19603e4c389c0871c518f3793e",
      "989fc16ce948433f9819d66ac80ee079",
      "fcab53707ac74228a51b0f6ee2411bbb",
      "08d346311b2e4868972f8cedc142c4a6",
      "d017c6b4a4ad4c9f947ffd79d4814186",
      "ce47ef904d9841d3aa3492c6813ccc0b",
      "061f33489ee74e7a9eb7609688194a7d",
      "e8409878e62d4767bf1d3dee1a85879b",
      "fc3327732878408498133920cf9a68da",
      "83dc4b5f5ca9458aa019b8130e7c1ae6",
      "ded232dd12f541e6bc4aa32369769396",
      "6a9b463a9e5b46a8a2662df4feed742b",
      "2fc482025fd14de09a7eac7614e544fb",
      "018588eb29cc48009e2f37b45186abf5",
      "7e64161897e64f90aa929ef6840c4dd3",
      "880c5713048e44189b0d5f63ab5aab31",
      "5c1e5e780d0246079b07ec2edb5bb0c4",
      "cc17108a6cd04818a1c6f75c58a91015",
      "6520fd7303804952a6e57ca1e7d5e47e",
      "fd3713f6cb924d07b3af41cbceaf7464",
      "aa1704f24f1a4d0693a85cc484e040c5",
      "78a7419c40944c9383f6b2eedd147cb8",
      "f85df026bfbe4e09b7cea55783f30956"
     ]
    },
    "id": "UQ8DtbPMXapM",
    "outputId": "91abceee-6368-47cc-e18e-07b8e8b6ca6d"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "import pycountry\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# -------------------------------------\n",
    "# 1. Setup: Country list (Arabic/English)\n",
    "# -------------------------------------\n",
    "def get_country_names():\n",
    "    countries = set()\n",
    "    for country in pycountry.countries:\n",
    "        names = [country.name]\n",
    "        if hasattr(country, 'official_name'):\n",
    "            names.append(country.official_name)\n",
    "        if hasattr(country, 'translations') and 'ar' in country.translations:\n",
    "            names.append(country.translations['ar'])\n",
    "        for name in names:\n",
    "            countries.add(name)\n",
    "    return countries\n",
    "\n",
    "COUNTRIES = get_country_names()\n",
    "\n",
    "# -------------------------------------\n",
    "# 2. Entities of Interest\n",
    "# -------------------------------------\n",
    "DISEASE_SYNONYMS = {\n",
    "    'كورونا': ['كورونا', 'فيروس كورونا', 'كوفيد', 'كوفيد19'],\n",
    "    'إنفلونزا': ['إنفلونزا', 'انفلونزا', 'إنفلونزا الخنازير', 'إنفلونزا الطيور'],\n",
    "    'جدري': ['جدري', 'الجدري'],\n",
    "    'إيبولا': ['إيبولا', 'الإيبولا'],\n",
    "    'السمنة': ['السمنة'],\n",
    "    'الفطر': ['الفطر', 'الفطر الأسود'],\n",
    "    'السيدا': ['السيدا', 'الإيدز', 'إيدز'],\n",
    "    'سرطان': ['سرطان', 'سرطان الثدي', 'سرطان الرئة', 'سرطان القولون'],\n",
    "    'القلب': ['القلب', 'أمراض القلب', 'ذبحة صدرية'],\n",
    "    'الملاريا': ['الملاريا'],\n",
    "    'الدرن': ['الدرن', 'السل'],\n",
    "    'التهاب الكبد': ['التهاب الكبد'],\n",
    "    'حمى الضنك': ['حمى الضنك']\n",
    "}\n",
    "\n",
    "VACCINE_KEYWORDS = [\n",
    "    'لقاح كورونا', 'لقاح فايزر', 'لقاح موديرنا', 'لقاح سينوفارم',\n",
    "    'فايزر', 'موديرنا', 'جونسون', 'أسترازينيكا', 'سبوتنيك', 'سينوفارم'\n",
    "]\n",
    "\n",
    "CONTINENTS = [\n",
    "    'أفريقيا', 'إفريقيا', 'Africa',\n",
    "    'آسيا', 'اسيا', 'Asia',\n",
    "    'أوروبا', 'اوروبا', 'Europe',\n",
    "    'أمريكا الشمالية', 'امريكا الشمالية', 'North America',\n",
    "    'أمريكا الجنوبية', 'امريكا الجنوبية', 'South America',\n",
    "    'أستراليا', 'استراليا', 'Australia',\n",
    "    'أنتاركتيكا', 'Antarctica', 'القارة القطبية الجنوبية'\n",
    "]\n",
    "\n",
    "# Reverse mapping for diseases\n",
    "alias_to_disease = {alias: canonical for canonical, aliases in DISEASE_SYNONYMS.items() for alias in aliases}\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Utility Functions\n",
    "# -------------------------------------\n",
    "def is_arabic(text):\n",
    "    return bool(re.search(r'[\\u0600-\\u06FF]', text))\n",
    "\n",
    "def get_country_from_location(location):\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": location, \"format\": \"json\", \"addressdetails\": 1, \"accept-language\": \"ar\"}\n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers={\"User-Agent\": \"entity-extractor\"})\n",
    "        data = r.json()\n",
    "        if data and \"address\" in data[0] and \"country\" in data[0][\"address\"]:\n",
    "            return data[0][\"address\"][\"country\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_edge_type(t1, t2):\n",
    "    types = sorted([t1, t2])\n",
    "    return \"-\".join(types)\n",
    "\n",
    "# -------------------------------------\n",
    "# 4. Load NER model\n",
    "# -------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 5. Process summaries & build graph\n",
    "# -------------------------------------\n",
    "nodes = {}  # label -> type\n",
    "edges = defaultdict(int)\n",
    "node_label_to_index = {}\n",
    "\n",
    "with open('merged_summaries.csv', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        summary = row.get('Summary', '').strip()\n",
    "        if not summary or summary.lower().startswith(\"empty\"):\n",
    "            continue\n",
    "\n",
    "        entities = ner(summary)\n",
    "        found_labels = set()\n",
    "\n",
    "        for ent in entities:\n",
    "            label = ent['word'].strip()\n",
    "            if not is_arabic(label) or label.startswith(\"##\"):\n",
    "                continue\n",
    "\n",
    "            # Generalize all vaccines to 'لقاح'\n",
    "            if label in VACCINE_KEYWORDS:\n",
    "                nodes['لقاح'] = 'vaccine'\n",
    "                found_labels.add('لقاح')\n",
    "                continue\n",
    "\n",
    "            if label in COUNTRIES:\n",
    "                nodes[label] = 'country'\n",
    "                found_labels.add(label)\n",
    "            elif label in CONTINENTS:\n",
    "                nodes[label] = 'continent'\n",
    "                found_labels.add(label)\n",
    "            elif ent['entity_group'] == \"LOC\":\n",
    "                country = get_country_from_location(label)\n",
    "                if country and is_arabic(country) and country not in alias_to_disease and country not in CONTINENTS:\n",
    "                    nodes[country] = 'country'\n",
    "                    found_labels.add(country)\n",
    "\n",
    "        for alias, canonical in alias_to_disease.items():\n",
    "            if alias in summary:\n",
    "                nodes[canonical] = 'disease'\n",
    "                found_labels.add(canonical)\n",
    "\n",
    "        for continent in CONTINENTS:\n",
    "            if continent in summary:\n",
    "                nodes[continent] = 'continent'\n",
    "                found_labels.add(continent)\n",
    "\n",
    "        for src, tgt in combinations(sorted(found_labels), 2):\n",
    "            if src == tgt:\n",
    "                continue\n",
    "            t1, t2 = nodes.get(src), nodes.get(tgt)\n",
    "            if not t1 or not t2:\n",
    "                continue\n",
    "            etype = get_edge_type(t1, t2)\n",
    "            edges[(src, tgt, etype)] += 1\n",
    "\n",
    "# -------------------------------------\n",
    "# 6. Assign index and write CSVs\n",
    "# -------------------------------------\n",
    "with open('nodes.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'label', 'type'])\n",
    "    for idx, (label, typ) in enumerate(nodes.items()):\n",
    "        node_label_to_index[label] = idx\n",
    "        writer.writerow([idx, label, typ])\n",
    "\n",
    "with open('edges.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['source', 'target', 'type', 'weight'])\n",
    "    for (src, tgt, etype), weight in edges.items():\n",
    "        if src in node_label_to_index and tgt in node_label_to_index:\n",
    "            writer.writerow([\n",
    "                node_label_to_index[src],\n",
    "                node_label_to_index[tgt],\n",
    "                etype,\n",
    "                weight\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz-j_0g1xQ2b"
   },
   "source": [
    "6-GCN LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqe4eS1IYBFL",
    "outputId": "631b9b3a-fa84-411f-a11f-ddb0e120206b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load data\n",
    "nodes_df = pd.read_csv(\"nodes.csv\")\n",
    "edges_df = pd.read_csv(\"edges.csv\")\n",
    "summaries_df = pd.read_csv(\"merged_summaries.csv\")\n",
    "summaries_df['Summary'] = summaries_df['Summary'].fillna(\"\")\n",
    "\n",
    "# Create mappings\n",
    "node_types = dict(zip(nodes_df['id'], nodes_df['type']))\n",
    "label_to_id = dict(zip(nodes_df['label'], nodes_df['id']))\n",
    "num_nodes = len(nodes_df)\n",
    "\n",
    "# Build node-to-summary map\n",
    "node_to_summaries = defaultdict(list)\n",
    "for idx, row in summaries_df.iterrows():\n",
    "    summary = row['Summary']\n",
    "    for label, node_id in label_to_id.items():\n",
    "        if isinstance(label, str) and re.search(rf'\\b{re.escape(label)}\\b', summary, re.UNICODE):\n",
    "            node_to_summaries[node_id].append(idx)\n",
    "\n",
    "# Edge list\n",
    "edges = list(edges_df[['source', 'target']].itertuples(index=False, name=None))\n",
    "edge_weights = edges_df['weight'].tolist()\n",
    "\n",
    "# Define edge type encoding\n",
    "def encode_edge_type(t: str) -> int:\n",
    "    t = t.lower()\n",
    "    if \"country\" in t and \"disease\" in t:\n",
    "        return 1\n",
    "    elif \"country\" in t and \"vaccine\" in t:\n",
    "        return 2\n",
    "    elif \"continent\" in t and (\"disease\" in t or \"vaccine\" in t):\n",
    "        return 4\n",
    "    elif \"country\" in t and \"country\" in t:\n",
    "        return 3\n",
    "    return 0\n",
    "\n",
    "edge_types = [encode_edge_type(rel_type) for rel_type in edges_df['type']]\n",
    "\n",
    "# Prepare adjacency and training edges\n",
    "adj_edges = []\n",
    "train_edges = []\n",
    "train_weights = []\n",
    "train_types = []\n",
    "\n",
    "for i, (a, b) in enumerate(edges):\n",
    "    t = edge_types[i]\n",
    "    adj_edges.append((a, b))\n",
    "\n",
    "    # Include country-disease, country-vaccine, and continent-disease links for prediction\n",
    "    if (\n",
    "        t in [1, 2,4]\n",
    "    ):\n",
    "        train_edges.append((a, b))\n",
    "        train_weights.append(edge_weights[i])\n",
    "        train_types.append(t)\n",
    "\n",
    "print(f\"Total edges in adjacency: {len(adj_edges)}\")\n",
    "print(f\"Positive edges for training (types 1, 2, or 4): {len(train_edges)}\")\n",
    "\n",
    "# Generate negative edges\n",
    "all_possible = list(combinations(range(num_nodes), 2))\n",
    "existing_set = set(train_edges)\n",
    "negative_edges = [e for e in all_possible if e not in existing_set]\n",
    "neg_sample = random.sample(negative_edges, min(len(train_edges), len(negative_edges)))\n",
    "neg_types = [0] * len(neg_sample)\n",
    "\n",
    "# Combine pos + neg\n",
    "all_edges = train_edges + neg_sample\n",
    "all_labels = [1] * len(train_edges) + [0] * len(neg_sample)\n",
    "all_types = train_types + neg_types\n",
    "\n",
    "# GCN Model\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden1, hidden2, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden1)\n",
    "        self.conv2 = GCNConv(hidden1, hidden2)\n",
    "        self.classifier = torch.nn.Linear(hidden2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data, edge_index):\n",
    "        x = self.conv1(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        src, dst = edge_index\n",
    "        edge_feats = x[src] * x[dst]\n",
    "        return torch.sigmoid(self.classifier(edge_feats).squeeze())\n",
    "# Training Loop\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "dropout = 0.4\n",
    "learning_rate = 0.05\n",
    "hidden_dims = (32, 16)\n",
    "patience = 151\n",
    "\n",
    "f1_scores, precisions, recalls, accuracies, aucs = [], [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(folds.split(all_edges, all_types)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_edges_fold = [all_edges[i] for i in train_idx]\n",
    "    train_labels = torch.tensor([all_labels[i] for i in train_idx], dtype=torch.float)\n",
    "    test_edges = [all_edges[i] for i in test_idx]\n",
    "    test_labels = torch.tensor([all_labels[i] for i in test_idx], dtype=torch.float)\n",
    "\n",
    "    # TF-IDF feature extraction (fold-wise)\n",
    "    train_summary_ids = set()\n",
    "    for edge in train_edges_fold:\n",
    "        for node in edge:\n",
    "            train_summary_ids.update(node_to_summaries.get(node, []))\n",
    "    train_summaries = [summaries_df['Summary'][i] for i in train_summary_ids]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    tfidf_matrix = vectorizer.fit_transform(train_summaries).toarray() if train_summaries else np.zeros((1, 100))\n",
    "    summary_idx_map = {idx: i for i, idx in enumerate(train_summary_ids)}\n",
    "\n",
    "    node_features = np.zeros((num_nodes, tfidf_matrix.shape[1]))\n",
    "    for node_id in range(num_nodes):\n",
    "        indices = [i for i in node_to_summaries[node_id] if i in summary_idx_map]\n",
    "        if indices:\n",
    "            vectors = [tfidf_matrix[summary_idx_map[i]] for i in indices]\n",
    "            node_features[node_id] = np.mean(vectors, axis=0)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    node_features = torch.nan_to_num(node_features)\n",
    "\n",
    "    edge_index_full = torch.tensor(adj_edges, dtype=torch.long).t()\n",
    "    edge_attr_full = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    graph = Data(x=node_features, edge_index=edge_index_full, edge_attr=edge_attr_full)\n",
    "\n",
    "    train_edge_index = torch.tensor(train_edges_fold, dtype=torch.long).t()\n",
    "    test_edge_index = torch.tensor(test_edges, dtype=torch.long).t()\n",
    "\n",
    "    model = LinkPredictor(node_features.shape[1], *hidden_dims, dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(400):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(graph, train_edge_index)\n",
    "        loss = F.binary_cross_entropy(pred, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_scores = model(graph, test_edge_index)\n",
    "            preds = (val_scores > 0.5).float()\n",
    "            precision = precision_score(test_labels, preds, zero_division=0)\n",
    "            recall = recall_score(test_labels, preds, zero_division=0)\n",
    "            f1 = f1_score(test_labels, preds, zero_division=0)\n",
    "            acc = accuracy_score(test_labels, preds)\n",
    "            auc = roc_auc_score(test_labels, val_scores)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_metrics = (precision, recall, acc, auc)\n",
    "            best_preds = preds\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    p, r, acc, auc = best_metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, best_preds).ravel()\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1_scores.append(best_f1)\n",
    "    accuracies.append(acc)\n",
    "    aucs.append(auc)\n",
    "\n",
    "    print(f\"Precision: {p:.4f} | Recall: {r:.4f} | F1: {best_f1:.4f} | Acc: {acc:.4f} | AUC: {auc:.4f}\")\n",
    "    print(f\"Confusion: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "    print(f\"Fold {fold+1} - Test size: {len(test_labels)}\")\n",
    "\n",
    "# Final metrics\n",
    "print(\"\\n=== Cross-Validation Results ===\")\n",
    "print(f\"Avg Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Avg Recall:    {np.mean(recalls):.4f}\")\n",
    "print(f\"Avg F1 Score:  {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Avg Accuracy:  {np.mean(accuracies):.4f}\")\n",
    "print(f\"Avg AUC-ROC:   {np.mean(aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8w55_lQxUTh"
   },
   "source": [
    "\n",
    "\n",
    "7-AGENT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDQYgqqB_61d",
    "outputId": "2b877791-96f7-4e15-b6c3-037b2b16fbcc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from groq import Groq\n",
    "from collections import Counter\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='inverse_relation_generation.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# API Key\n",
    "client = Groq(api_key=\"\")\n",
    "\n",
    "# Load edges with weight\n",
    "df = pd.read_csv(\"edges.csv\")  # columns: source, target, type, weight\n",
    "logging.info(f\"Loaded {len(df)} edges from edges.csv\")\n",
    "\n",
    "# Function that calls the LLM to generate the inverse relation\n",
    "def generate_inverse_relation_en(source, target, rel_type, weight):\n",
    "    prompt = f\"\"\"Given a relationship between two entities in the format: source,target,type,weight\n",
    "The source and target are numeric IDs (not names). The weight is an integer.\n",
    "If the relationship can be reversed, return the reversed relationship in the format: source,target,type (do NOT include the weight in the output).\n",
    "Otherwise, return nothing.\n",
    "\n",
    "Examples:\n",
    "Input: 1,2,country-disease,5\n",
    "Output: 2,1,disease-country\n",
    "\n",
    "Input: 1,0,country-country,3\n",
    "Output: 0,1,country-country\n",
    "\n",
    "Input: 2,1,disease-country,2\n",
    "Output: 1,2,country-disease\n",
    "\n",
    "Input: 3,1,country-country,1\n",
    "Output: 1,3,country-country\n",
    "\n",
    "Now, given the following:\n",
    "{source},{target},{rel_type},{weight}\n",
    "Return only the reversed relationship in the format: source,target,type (do NOT include the weight).\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=60,\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        # Only keep the first line with three comma-separated parts\n",
    "        for line in result.splitlines():\n",
    "            parts = [x.strip() for x in line.split(\",\")]\n",
    "            if len(parts) == 3 and all(parts[:2]) and all(part.isdigit() for part in parts[:2]):\n",
    "                return \",\".join(parts)\n",
    "        logging.warning(f\"Invalid format from LLM for input {source},{target},{rel_type},{weight}: '{result}'\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM error for {source} → {target} ({rel_type}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Count reversed relations in the original file\n",
    "edge_counter = Counter(\n",
    "    (str(row[\"target\"]), str(row[\"source\"]), str(row[\"type\"]))\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "# Generate reversed relations with weight preserved\n",
    "inverse_edges = []\n",
    "for idx, row in df.iterrows():\n",
    "    source, target, rel_type, weight = row[\"source\"], row[\"target\"], row[\"type\"], row[\"weight\"]\n",
    "    if rel_type in [\"country-country\", \"continent-country\", \"disease-disease\", \"disease-vaccine\"]:\n",
    "        continue\n",
    "    logging.info(f\"Processing edge {idx}: {source} → {target} ({rel_type})\")\n",
    "    inverse = generate_inverse_relation_en(source, target, rel_type, weight)\n",
    "    if inverse:\n",
    "        parts = [x.strip() for x in inverse.split(\",\")]\n",
    "        if len(parts) == 3 and all(parts):\n",
    "            s2, t2, r2 = parts\n",
    "            inverse_edges.append({\n",
    "                \"source\": s2,\n",
    "                \"target\": t2,\n",
    "                \"type\": r2,\n",
    "                \"weight\": weight\n",
    "            })\n",
    "            logging.info(f\"Added inverse: {s2} → {t2} ({r2})\")\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# DataFrame of inverse edges\n",
    "df_inverse = pd.DataFrame(inverse_edges)\n",
    "df_inverse = df_inverse.dropna()\n",
    "df_inverse = df_inverse[(df_inverse[\"source\"] != \"\") & (df_inverse[\"target\"] != \"\") & (df_inverse[\"type\"] != \"\")]\n",
    "logging.info(f\"{len(df_inverse)} valid inverse edges generated.\")\n",
    "\n",
    "# Merge with original edges\n",
    "df_total = pd.concat([df, df_inverse], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Save\n",
    "df_total.to_csv(\"edges_with_inverses.csv\", index=False)\n",
    "logging.info(\"Saved combined edges to edges_with_inverses.csv\")\n",
    "print(\"File 'edges_with_inverses.csv' generated with weights and inverse relations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_nZSKMlAIJe"
   },
   "source": [
    "8-GCN link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXDHB3y6vHZT",
    "outputId": "b76fdaa0-0d1b-40a3-8658-d1c5775603ac"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load data\n",
    "nodes_df = pd.read_csv(\"nodes.csv\")\n",
    "edges_df = pd.read_csv(\"edges_with_inverses.csv\", dtype={\"source\": int, \"target\": int})\n",
    "summaries_df = pd.read_csv(\"merged_summaries.csv\")\n",
    "summaries_df['Summary'] = summaries_df['Summary'].fillna(\"\")\n",
    "\n",
    "# Create mappings\n",
    "node_types = dict(zip(nodes_df['id'], nodes_df['type']))\n",
    "label_to_id = dict(zip(nodes_df['label'], nodes_df['id']))\n",
    "num_nodes = len(nodes_df)\n",
    "\n",
    "# Build node-to-summary map\n",
    "node_to_summaries = defaultdict(list)\n",
    "for idx, row in summaries_df.iterrows():\n",
    "    summary = row['Summary']\n",
    "    for label, node_id in label_to_id.items():\n",
    "        if isinstance(label, str) and re.search(rf'\\b{re.escape(label)}\\b', summary, re.UNICODE):\n",
    "            node_to_summaries[node_id].append(idx)\n",
    "\n",
    "# Edge list\n",
    "edges = list(edges_df[['source', 'target']].itertuples(index=False, name=None))\n",
    "edge_weights = edges_df['weight'].tolist()\n",
    "\n",
    "# Define edge type encoding\n",
    "\n",
    "def encode_edge_type(t: str) -> int:\n",
    "    t = t.lower()\n",
    "    if \"country\" in t and \"disease\" in t:\n",
    "        return 1\n",
    "    elif \"country\" in t and \"vaccine\" in t:\n",
    "        return 2\n",
    "    elif \"continent\" in t and (\"disease\" in t or \"vaccine\" in t):\n",
    "        return 4\n",
    "    elif \"country\" in t and \"country\" in t:\n",
    "        return 3\n",
    "    return 0\n",
    "\n",
    "edge_types = [encode_edge_type(rel_type) for rel_type in edges_df['type']]\n",
    "\n",
    "# Prepare adjacency and training edges\n",
    "adj_edges = []\n",
    "train_edges = []\n",
    "train_weights = []\n",
    "train_types = []\n",
    "\n",
    "for i, (a, b) in enumerate(edges):\n",
    "    t = edge_types[i]\n",
    "    adj_edges.append((a, b))\n",
    "\n",
    "    # Include country-disease, country-vaccine, and continent-disease links for prediction\n",
    "    if t in [1, 2, 4]:\n",
    "        train_edges.append((a, b))\n",
    "        train_weights.append(edge_weights[i])\n",
    "        train_types.append(t)\n",
    "\n",
    "print(f\"Total edges in adjacency: {len(adj_edges)}\")\n",
    "print(f\"Positive edges for training (types 1, 2, or 4): {len(train_edges)}\")\n",
    "\n",
    "# Generate negative edges\n",
    "all_possible = list(combinations(range(num_nodes), 2))\n",
    "existing_set = set(train_edges)\n",
    "negative_edges = [e for e in all_possible if e not in existing_set]\n",
    "neg_sample = random.sample(negative_edges, min(len(train_edges), len(negative_edges)))\n",
    "neg_types = [0] * len(neg_sample)\n",
    "\n",
    "# Combine pos + neg\n",
    "all_edges = train_edges + neg_sample\n",
    "all_labels = [1] * len(train_edges) + [0] * len(neg_sample)\n",
    "all_types = train_types + neg_types\n",
    "\n",
    "# GCN Model\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden1, hidden2, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden1)\n",
    "        self.conv2 = GCNConv(hidden1, hidden2)\n",
    "        self.classifier = torch.nn.Linear(hidden2, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data, edge_index):\n",
    "        x = self.conv1(data.x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index, edge_weight=data.edge_attr)\n",
    "        src, dst = edge_index\n",
    "        edge_feats = x[src] * x[dst]\n",
    "        return torch.sigmoid(self.classifier(edge_feats).squeeze())\n",
    "# Training Loop\n",
    "folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "dropout = 0.3\n",
    "learning_rate = 0.05\n",
    "hidden_dims = (128, 64)\n",
    "patience = 150\n",
    "\n",
    "f1_scores, precisions, recalls, accuracies, aucs = [], [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(folds.split(all_edges, all_types)):\n",
    "    print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "    train_edges_fold = [all_edges[i] for i in train_idx]\n",
    "    train_labels = torch.tensor([all_labels[i] for i in train_idx], dtype=torch.float)\n",
    "    test_edges = [all_edges[i] for i in test_idx]\n",
    "    test_labels = torch.tensor([all_labels[i] for i in test_idx], dtype=torch.float)\n",
    "\n",
    "    # TF-IDF feature extraction (fold-wise)\n",
    "    train_summary_ids = set()\n",
    "    for edge in train_edges_fold:\n",
    "        for node in edge:\n",
    "            train_summary_ids.update(node_to_summaries.get(node, []))\n",
    "    train_summaries = [summaries_df['Summary'][i] for i in train_summary_ids]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    tfidf_matrix = vectorizer.fit_transform(train_summaries).toarray() if train_summaries else np.zeros((1, 100))\n",
    "    summary_idx_map = {idx: i for i, idx in enumerate(train_summary_ids)}\n",
    "\n",
    "    node_features = np.zeros((num_nodes, tfidf_matrix.shape[1]))\n",
    "    for node_id in range(num_nodes):\n",
    "        indices = [i for i in node_to_summaries[node_id] if i in summary_idx_map]\n",
    "        if indices:\n",
    "            vectors = [tfidf_matrix[summary_idx_map[i]] for i in indices]\n",
    "            node_features[node_id] = np.mean(vectors, axis=0)\n",
    "\n",
    "    node_features = torch.tensor(node_features, dtype=torch.float)\n",
    "    node_features = torch.nan_to_num(node_features)\n",
    "\n",
    "    edge_index_full = torch.tensor(adj_edges, dtype=torch.long).t()\n",
    "    edge_attr_full = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    graph = Data(x=node_features, edge_index=edge_index_full, edge_attr=edge_attr_full)\n",
    "\n",
    "    train_edge_index = torch.tensor(train_edges_fold, dtype=torch.long).t()\n",
    "    test_edge_index = torch.tensor(test_edges, dtype=torch.long).t()\n",
    "\n",
    "    model = LinkPredictor(node_features.shape[1], *hidden_dims, dropout)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(400):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(graph, train_edge_index)\n",
    "        loss = F.binary_cross_entropy(pred, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_scores = model(graph, test_edge_index)\n",
    "            preds = (val_scores > 0.5).float()\n",
    "            precision = precision_score(test_labels, preds, zero_division=0)\n",
    "            recall = recall_score(test_labels, preds, zero_division=0)\n",
    "            f1 = f1_score(test_labels, preds, zero_division=0)\n",
    "            acc = accuracy_score(test_labels, preds)\n",
    "            auc = roc_auc_score(test_labels, val_scores)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_metrics = (precision, recall, acc, auc)\n",
    "            best_preds = preds\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    p, r, acc, auc = best_metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, best_preds).ravel()\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    f1_scores.append(best_f1)\n",
    "    accuracies.append(acc)\n",
    "    aucs.append(auc)\n",
    "\n",
    "    print(f\"Precision: {p:.4f} | Recall: {r:.4f} | F1: {best_f1:.4f} | Acc: {acc:.4f} | AUC: {auc:.4f}\")\n",
    "    print(f\"Confusion: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "    print(f\"Fold {fold+1} - Test size: {len(test_labels)}\")\n",
    "\n",
    "# Final metrics\n",
    "print(\"\\n=== Cross-Validation Results ===\")\n",
    "print(f\"Avg Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Avg Recall:    {np.mean(recalls):.4f}\")\n",
    "print(f\"Avg F1 Score:  {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Avg Accuracy:  {np.mean(accuracies):.4f}\")\n",
    "print(f\"Avg AUC-ROC:   {np.mean(aucs):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
